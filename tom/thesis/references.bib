@book{Neilsen2015,
author = {Neilsen, Michael},
keywords = {nnDeep},
mendeley-tags = {nnDeep},
publisher = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2015}
}
@book{Grunwald2010,
author = {Grunwald, Peter and Spirtes, Peter. and Bilmes, Jeff. and {Google (Firm)} and {Association for Uncertainty in Artificial Intelligence.}},
booktitle = {Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
isbn = {9780974903965},
pages = {753},
publisher = {AUAI Press},
title = {{Uncertainty in artificial intelligence : proceedings of the Twenty-sixth Conference (2010), June 8-11, 2010, Avalon, CA}},
url = {http://dl.acm.org/citation.cfm?id=3023618},
year = {2010}
}
@article{Audibert2009,
abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit prob-lem. Concretely, we remove an extraneous loga-rithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate log-arithmic in the number of plays.},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Audibert, Bubeck - 2009 - Minimax policies for adversarial and stochastic bandits.pdf:pdf},
title = {{Minimax policies for adversarial and stochastic bandits}},
url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/COLT09{\_}AB.pdf},
year = {2009}
}
@inproceedings{Auer1995,
author = {Auer, P. and Cesa-Bianchi, N. and Freund, Y. and Schapire, R.E.},
booktitle = {Proceedings of IEEE 36th Annual Foundations of Computer Science},
doi = {10.1109/SFCS.1995.492488},
isbn = {0-8186-7183-1},
pages = {322--331},
publisher = {IEEE Comput. Soc. Press},
title = {{Gambling in a rigged casino: The adversarial multi-armed bandit problem}},
url = {http://ieeexplore.ieee.org/document/492488/},
year = {1995}
}
@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, Peter and Fischer, Paul},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auer, Fischer - 2002 - Finite-time Analysis of the Multiarmed Bandit Problem.pdf:pdf},
journal = {Machine Learning},
keywords = {adaptive allocation rules,bandit problems,finite horizon regret},
pages = {235--256},
title = {{Finite-time Analysis of the Multiarmed Bandit Problem*}},
url = {https://d2925a48-a-62cb3a1a-s-sites.googlegroups.com/site/anrexplora/bibliography/fta-2002.pdf?attachauth=ANoY7coeZC0HlDGNM8HV9yv-Ortdkr4PV0f4s4ACpG6jD5frf1k4uI4h4UOvKE74sUf7bshbEXqaTwCw0n-5QBDvrQgw6Q6VLAzWNLQlWmI{\_}{\_}O4CnzTwZDM9ed3nEa9h1haSb-Hyz6N1nAT7dSee3},
volume = {47},
year = {2002}
}
@inproceedings{Gelly2007,
address = {New York, New York, USA},
author = {Gelly, Sylvain and Silver, David},
booktitle = {Proceedings of the 24th international conference on Machine learning - ICML '07},
doi = {10.1145/1273496.1273531},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelly, Silver - 2007 - Combining online and offline knowledge in UCT.pdf:pdf},
isbn = {9781595937933},
pages = {273--280},
publisher = {ACM Press},
title = {{Combining online and offline knowledge in UCT}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273531},
year = {2007}
}
@article{Gelly2006,
author = {Gelly, Sylvain and Wang, Yizao and Munos, R{\'{e}}mi and Teytaud, Olivier},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelly et al. - 2006 - Modification of UCT with Patterns in Monte-Carlo Go.pdf:pdf},
title = {{Modification of UCT with Patterns in Monte-Carlo Go}},
url = {https://hal.inria.fr/inria-00117266v3},
year = {2006}
}
@article{Gelly2012,
abstract = {abstract The ancient oriental game of Go has long been considered a grand challenge for artificial intelligence. For decades, computer Go has defied the classical methods in game tree search that worked so successfully for chess and checkers. However, recent play in computer Go has been transformed by a new paradigm for tree search based on Monte-Carlo methods. Programs based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players. In this paper, we describe the lead-ing algorithms for Monte-Carlo tree search and explain how they have advanced the state of the art in computer Go. 1. inTRoDucTion Sequential decision making has been studied in a num-ber of fields, ranging from optimal control through opera-tions research to artificial intelligence (AI). The challenge of sequential decision making is to select actions that maxi-mize some long-term objective (e.g., winning a game), when the consequences of those actions may not be revealed for many steps. In this paper, we focus on a new approach to sequential decision making that was developed recently in the context of two-player games. Classic two-player games are excellent test beds for AI. They provide closed micro-worlds with simple rules that have been selected and refined over hundreds or thousands of years to challenge human players. They also provide clear benchmarks of performance both between different pro-grams and against human intelligence. In two-player games such as chess, checkers, othello, and backgammon, human levels of performance have been exceeded by programs that combine brute force tree-search with human knowledge or reinforcement learning.},
author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Mich{\`{e}}le and Silver, David and Szepesv{\'{a}}ri, Csaba and Teytaud, Olivier},
doi = {10.1145/2093548.2093574},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelly et al. - 2012 - The Grand Challenge of Computer Go Monte Carlo Tree Search and Extensions.pdf:pdf},
number = {3},
title = {{The Grand Challenge of Computer Go: Monte Carlo Tree Search and Extensions}},
url = {http://delivery.acm.org/10.1145/2100000/2093574/p106-gelly.pdf?ip=147.188.254.221{\&}id=2093574{\&}acc=ACTIVE SERVICE{\&}key=BF07A2EE685417C5.426E5CB647D49637.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=802598993{\&}CFTOKEN=66784509{\&}{\_}{\_}acm{\_}{\_}=1503920618{\_}3dff4b75dd84e3d34976},
volume = {55},
year = {2012}
}
@book{Russell2016,
abstract = {Third edition "Global edition"--Umschlag},
author = {Russell, Stuart J. and Norvig, Peter},
isbn = {1292153962},
keywords = {ai},
mendeley-tags = {ai},
publisher = {Pearson},
title = {{Artificial intelligence a modern approach}},
year = {2016}
}
@article{Helmbold2009,
abstract = {— We present and explore the effectiveness of sev-eral variations on the All-Moves-As-First (AMAF) heuristic in Monte-Carlo Go. Our results show that: • Random play-outs provide more information about the goodness of moves made earlier in the play-out. • AMAF updates are not just a way to quickly initialize counts, they are useful after every play-out. • Updates even more aggressive than AMAF can be even more beneficial.},
author = {Helmbold, David P and Parker-Wood, Aleatha},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Helmbold, Parker-Wood - 2009 - All-Moves-As-First Heuristics in Monte-Carlo Go.pdf:pdf},
keywords = {All-Moves-As-First,Computer Go,Heuristic search,Monte-Carlo Tree Search,UCT},
pages = {605--610},
title = {{All-Moves-As-First Heuristics in Monte-Carlo Go}},
url = {https://users.soe.ucsc.edu/{~}dph/mypubs/AMAFpaperWithRef.pdf},
year = {2009}
}
@article{Browne2012,
abstract = {—Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
author = {Browne, Cameron and Powley, Edward and Whitehouse, Daniel and Lucas, Simon and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
doi = {10.1109/TCIAIG.2012.2186810},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - Unknown - A Survey of Monte Carlo Tree Search Methods.pdf:pdf},
journal = {Artificial Intelligence},
number = {1},
pages = {51},
title = {{A Survey of Monte Carlo Tree Search Methods}},
url = {http://www.cameronius.com/cv/mcts-survey-master.pdf},
volume = {4},
year = {2012}
}
