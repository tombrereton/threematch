@article{Coquelin2007,
abstract = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go [Gelly et al., 2006]. Their efficient exploration of the tree enables to return rapidly a good value, and improve precision if more time is provided. The UCT algorithm [Kocsis and Szepesvari, 2006], a tree search method based on Upper Confidence Bounds (UCB) [Auer et al. 2002], is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is ''over-optimistic'' in some sense, leading to a worst-case regret that may be very poor. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially in the horizon depth is analyzed. We then consider Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes into account actual smoothness of the rewards for performing efficient ''cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree expansion which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, only the optimal branches are indefinitely developed. We illustrate these methods on a global optimization problem of a continuous function, given noisy values.},
author = {Coquelin, Pierre-Arnaud and Munos, R{\'{e}}mi},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Coquelin, Munos - 2007 - Bandit Algorithms for Tree Search.pdf:pdf},
keywords = {Bandit algorithm Tree search Concentration inequality Stochastic optimization},
title = {{Bandit Algorithms for Tree Search}},
url = {https://hal.inria.fr/inria-00150207},
year = {2007}
}
@book{Osborne1994,
author = {Osborne, Martin J. and Rubinstein, Ariel.},
isbn = {0262650401},
pages = {352},
publisher = {MIT Press},
title = {{A course in game theory}},
year = {1994}
}
@article{Pratt1964,
abstract = {This paper concerns utility functions for money. A measure of risk aversion in the small, the risk premium or insurance premium for an arbitrary risk, and a natural concept of decreasing risk aversion are discussed and related to one another. Risks are also considered as a proportion of total assets.},
author = {Pratt, John W.},
doi = {10.2307/1913738},
issn = {00129682},
journal = {Econometrica},
keywords = {Cash equivalents,Concavity,Decreasing functions,Economic theory,Financial risk,Insurance premiums,Probability distributions,Risk aversion,Risk premiums,Utility functions},
month = {jan},
number = {1/2},
pages = {122},
publisher = {The Econometric Society},
title = {{Risk Aversion in the Small and in the Large}},
url = {http://www.jstor.org/stable/1913738?origin=crossref},
volume = {32},
year = {1964}
}
@article{Kaelbling1998,
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPs) and partially observable MDPs (POMDPs). We then outline a novel algorithm for solving POMDPs off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPs, and of some possibilities for finding approximate solutions. Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a corner; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? PII: S 0 0 0 4 -3 7 0 2 (9 8) 0 0 0 2 3 -X 100 L.P. Kaelbling et al. / Artificial Intelligence 101 (1998) 99â€“134 In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the north-east corner of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that fulfill both purposes simultaneously.},
author = {Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaelbling, Littman, Cassandra - 1998 - Planning and acting in partially observable stochastic domains.pdf:pdf},
journal = {Artificial Intelligence},
keywords = {Partially observable Markov decision processes,Planning,Uncertainty},
pages = {99--134},
title = {{Planning and acting in partially observable stochastic domains}},
url = {http://people.csail.mit.edu/lpk/papers/aij98-pomdp.pdf},
volume = {101},
year = {1998}
}
@book{Neilsen2015,
author = {Neilsen, Michael},
keywords = {nnDeep},
mendeley-tags = {nnDeep},
publisher = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2015}
}
@book{Grunwald2010,
author = {Grunwald, Peter and Spirtes, Peter. and Bilmes, Jeff. and {Google (Firm)} and {Association for Uncertainty in Artificial Intelligence.}},
booktitle = {Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
isbn = {9780974903965},
pages = {753},
publisher = {AUAI Press},
title = {{Uncertainty in artificial intelligence : proceedings of the Twenty-sixth Conference (2010), June 8-11, 2010, Avalon, CA}},
url = {http://dl.acm.org/citation.cfm?id=3023618},
year = {2010}
}
@article{Audibert2009,
abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit prob-lem. Concretely, we remove an extraneous loga-rithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate log-arithmic in the number of plays.},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Audibert, Bubeck - 2009 - Minimax policies for adversarial and stochastic bandits.pdf:pdf},
title = {{Minimax policies for adversarial and stochastic bandits}},
url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/COLT09{\_}AB.pdf},
year = {2009}
}
@inproceedings{Auer1995,
author = {Auer, P. and Cesa-Bianchi, N. and Freund, Y. and Schapire, R.E.},
booktitle = {Proceedings of IEEE 36th Annual Foundations of Computer Science},
doi = {10.1109/SFCS.1995.492488},
isbn = {0-8186-7183-1},
pages = {322--331},
publisher = {IEEE Comput. Soc. Press},
title = {{Gambling in a rigged casino: The adversarial multi-armed bandit problem}},
url = {http://ieeexplore.ieee.org/document/492488/},
year = {1995}
}
@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, Peter and Fischer, Paul},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auer, Fischer - 2002 - Finite-time Analysis of the Multiarmed Bandit Problem.pdf:pdf},
journal = {Machine Learning},
keywords = {adaptive allocation rules,bandit problems,finite horizon regret},
pages = {235--256},
title = {{Finite-time Analysis of the Multiarmed Bandit Problem*}},
url = {https://link.springer.com/article/10.1023/A:1013689704352},
volume = {47},
year = {2002}
}
@inproceedings{Gelly2007,
address = {New York, New York, USA},
author = {Gelly, Sylvain and Silver, David},
booktitle = {Proceedings of the 24th international conference on Machine learning - ICML '07},
doi = {10.1145/1273496.1273531},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelly, Silver - 2007 - Combining online and offline knowledge in UCT.pdf:pdf},
isbn = {9781595937933},
pages = {273--280},
publisher = {ACM Press},
title = {{Combining online and offline knowledge in UCT}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273531},
year = {2007}
}
@article{Gelly2006,
author = {Gelly, Sylvain and Wang, Yizao and Munos, R{\'{e}}mi and Teytaud, Olivier},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelly et al. - 2006 - Modification of UCT with Patterns in Monte-Carlo Go.pdf:pdf},
title = {{Modification of UCT with Patterns in Monte-Carlo Go}},
url = {https://hal.inria.fr/inria-00117266v3},
year = {2006}
}
@article{Gelly2012,
abstract = {abstract The ancient oriental game of Go has long been considered a grand challenge for artificial intelligence. For decades, computer Go has defied the classical methods in game tree search that worked so successfully for chess and checkers. However, recent play in computer Go has been transformed by a new paradigm for tree search based on Monte-Carlo methods. Programs based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players. In this paper, we describe the lead-ing algorithms for Monte-Carlo tree search and explain how they have advanced the state of the art in computer Go. 1. inTRoDucTion Sequential decision making has been studied in a num-ber of fields, ranging from optimal control through opera-tions research to artificial intelligence (AI). The challenge of sequential decision making is to select actions that maxi-mize some long-term objective (e.g., winning a game), when the consequences of those actions may not be revealed for many steps. In this paper, we focus on a new approach to sequential decision making that was developed recently in the context of two-player games. Classic two-player games are excellent test beds for AI. They provide closed micro-worlds with simple rules that have been selected and refined over hundreds or thousands of years to challenge human players. They also provide clear benchmarks of performance both between different pro-grams and against human intelligence. In two-player games such as chess, checkers, othello, and backgammon, human levels of performance have been exceeded by programs that combine brute force tree-search with human knowledge or reinforcement learning.},
author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Mich{\`{e}}le and Silver, David and Szepesv{\'{a}}ri, Csaba and Teytaud, Olivier},
doi = {10.1145/2093548.2093574},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelly et al. - 2012 - The Grand Challenge of Computer Go Monte Carlo Tree Search and Extensions.pdf:pdf},
number = {3},
title = {{The Grand Challenge of Computer Go: Monte Carlo Tree Search and Extensions}},
url = {http://dl.acm.org/citation.cfm?id=2093574},
volume = {55},
year = {2012}
}
@book{Russell2016,
abstract = {Third edition "Global edition"--Umschlag},
author = {Russell, Stuart J. and Norvig, Peter},
isbn = {1292153962},
keywords = {ai},
mendeley-tags = {ai},
publisher = {Pearson},
title = {{Artificial intelligence a modern approach}},
year = {2016}
}
@article{Helmbold2009,
abstract = {â€” We present and explore the effectiveness of sev-eral variations on the All-Moves-As-First (AMAF) heuristic in Monte-Carlo Go. Our results show that: â€¢ Random play-outs provide more information about the goodness of moves made earlier in the play-out. â€¢ AMAF updates are not just a way to quickly initialize counts, they are useful after every play-out. â€¢ Updates even more aggressive than AMAF can be even more beneficial.},
author = {Helmbold, David P and Parker-Wood, Aleatha},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Helmbold, Parker-Wood - 2009 - All-Moves-As-First Heuristics in Monte-Carlo Go.pdf:pdf},
keywords = {All-Moves-As-First,Computer Go,Heuristic search,Monte-Carlo Tree Search,UCT},
pages = {605--610},
title = {{All-Moves-As-First Heuristics in Monte-Carlo Go}},
url = {https://users.soe.ucsc.edu/{~}dph/mypubs/AMAFpaperWithRef.pdf},
year = {2009}
}
@article{Browne2012,
abstract = {â€”Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
author = {Browne, Cameron and Powley, Edward and Whitehouse, Daniel and Lucas, Simon and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
doi = {10.1109/TCIAIG.2012.2186810},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - Unknown - A Survey of Monte Carlo Tree Search Methods.pdf:pdf},
journal = {Artificial Intelligence},
number = {1},
pages = {51},
title = {{A Survey of Monte Carlo Tree Search Methods}},
url = {http://www.cameronius.com/cv/mcts-survey-master.pdf},
volume = {4},
year = {2012}
}
